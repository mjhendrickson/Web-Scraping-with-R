---
title: "Web Scraping in R"
subtitle: ""  
author: 
  - "Dr. Matthew Hendrickson"
date: "July 9, 2020"
output:
  revealjs::revealjs_presentation:
    theme: night
    center: true
    widescreen: true
    incremental: true
    fig_width: 9
    fig_height: 3
    df_print: paged
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, comment = "#>", collapse = TRUE)
library(tidyverse)
library(robotstxt)
library(rvest)
```

# ITEMS TO FIX:

1. Author
2. Rating Count (works, clean it up)
3. Scraping Methods
4. Data Assembly
5. A Little About Web Scraping
6. Multiple format & price entries





# Topics

Ensure these are updated

1. About Me
2. A Little About Web Scraping
3. Robots.txt
4. HTML Structure
5. A Little Help with CSS
6. Scraping Methods (HTML & XPATH)
7. The Setup
8. Scraping the Data
9. Assembling the Data
10. References & Resources





# About Me

<div style="float: left; width: 50%;">
<img src = "images/headshot.png" width="450" height="450">
</div>

<div style="float: left; width: 50%;">
</br>

- Social Scientist by Training
     - Psychology & Music `%>%`
     - More Psychology `%>%`
     - Law & Policy
- Higher Education Analyst by Trade
- Independent Consultant
- 13 years in analytics
</div>





# A Little About Web Scraping

"Web scraping is the process of automatically mining data or collecting information from the World Wide Web." -- Wikipedia

## Purpose

Web scraping is a flexible method to extract data from the internet. It can involve extracting numerical or text data.

## Use Cases

There are many uses for web scraping, including but not limited it:
   a. Price monitoring
   b. Sentiment analysis
   c. Time series tracking and analysis

## Robots File

Always ensure - PRIOR to scraping - that you have rights to scrape the website. This is critical as you can be blocked from sites or even face legal action. This can easily be accomplished with the `robotstxt` package.





# Robots.txt

Always ensure you check the robots.txt file! This assures you are not breaking the terms of service by scraping the site.

```{r robots, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
paths_allowed(paths = c("https://netflix.com/"))
```

This example shows that Netflix does not allow you to scrape their site.





# HTML Structure

<img src = "images/html-structure.png">

<p style = "font-size:65%;">Image credit: [Professor Shawn Santo](http://www2.stat.duke.edu/~fl35/teaching/440-19F/decks/webscraping2.html#3)</p>





# A Little Help with CSS

"HTML is the standard markup language for creating Web pages." -- W3Schools

"CSS describes how HTML elements are to be displayed on screen, paper, or in other media." -- W3Schools

If you aren't familiar with CSS, extracting parts of a website can be daunting.

SelectorGadget is incredibly helpful for this purpose.

<https://selectorgadget.com/>

* This is a Chrome only extension. Another option is to inspect the page elements.





# HTML Tags

HTML is strucutred with "tags." These tags indicate portions of the page and can be called by their structure.

There are many types of tags - here are some important ones for scraping:
* `<h1>` - header tags
* `<p>` - paragraph elements
* `<ul>` - unordered bulleted list
* `<ol>` - ordered list
* `<li>` - individual list item
* `<div>` - division
* `<table>` - table





# Web Scraping





# Scraping Methods

HTML - syntax is easier and aligns with HTML tags

XPATH - useful when the node isn't uniquely identified with CSS





# The Setup

Set up the environment to scrap the site.

```{r setup_slide, eval = FALSE}
library(tidyverse)
library(robotstxt)
library(rvest)
```

That's it! These are all the tools you'll need.





# Determine a website to scrape

It only seems appropriate to pull data from Amazon regarding R books

Ensure we can scrape the site

```{r robots_amazon, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
paths_allowed(paths = c("https://amazon.com/"))
```



We are good to scrape!





# Setting the URLs

Before you can get started, you must specific the URLs to pass to the function.

```{r amazon_html, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon <- read_html("https://www.amazon.com/s?k=R&i=stripbooks&rh=n%3A283155%2Cn%3A75%2Cn%3A13983&dc&qid=1592086532&rnid=1000&ref=sr_nr_n_1")
# amazon_pg_2 <- read_html("https://www.amazon.com/s?k=R&i=stripbooks&rh=n%3A283155%2Cn%3A75%2Cn%3A13983&dc&page=2&qid=1592086539&rnid=1000&ref=sr_pg_2")
```





# Scraping Book Titles

```{r amazon_title, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".s-line-clamp-2") %>% 
  html_text() -> amazon_titles
head(amazon_titles)
```

The element pulls a number of breaks and blank spaces.

Let's clean this up with `str_trim`.

---

# The titles have a great deal of white space and breaks (\n), these need to be removed

```{r amazon_title_clean, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_titles <- str_trim(amazon_titles) # Removes leading & training space
head(amazon_titles)
```

This simple function returns cleaned text.

Unfortunately, there are mutilple formats and prices for each title. Here's we'll create additional title records for creating the dataset.

---

```{r amazon_title_duplicate, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#amazon_titles %>% 
  #append(values = NA, after = 1) -> amazon_titles
#head(amazon_titles)
```

This structure will serve our needs for future steps.





# Get the book author(s)

```{r amazon_author, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
#  html_nodes("a.a-size-base.a-link-normal") %>% 
  html_nodes("div.a-row.a-size-base.a-color-secondary") %>% 
  html_text() %>% 
  str_trim() -> amazon_authors
head(amazon_authors)
```

We run into issues here due to the element in which authors are placed.

We also need to split the author fields.





# Get the book format

```{r format, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("a.a-size-base.a-link-normal.a-text-bold") %>% 
  html_text() %>% 
  str_trim() -> amazon_format
head(amazon_format)
```





# Get the book price

```{r price_whole, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".a-price-whole") %>% 
  html_text() -> amazon_price_whole
head(amazon_price_whole)
```

The price structure splits price into two elements. We must pull each and combine them into a single price.

# Get (the rest of) the book price

```{r price_fraction, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes(".a-price-fraction") %>% 
  html_text() -> amazon_price_fraction
head(amazon_price_fraction)
```

# Combine price portions

```{r price_total, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_price <- as.numeric(paste(amazon_price_whole, amazon_price_fraction, sep = ""))
head(amazon_price)
```





# Get the book rating

```{r rate, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("i.a-icon.a-icon-star-small.a-star-small-4-5.aok-align-bottom") %>% 
  html_text() -> amazon_rating
head(amazon_rating)
```

Let's trim this into a usable metric.

```{r rate_trim, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_rating <- as.numeric(substr(amazon_rating, 1, 3))
head(amazon_rating)
```

Another issue with ratings is that not all titles have ratings. We must adjust for this.

I'll do this manually by appending NA where ratings are missing.

```{r rate_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_rating %>% 
  append(values = NA, after = 1) %>% 
  append(values = NA, after = 11) -> amazon_rating
head(amazon_rating)
```





# Get the book rating count

We'll run into the same issue as the actual rating. But first, we'll also see a number of cleaning steps are needed.

```{r rate_n, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
#  html_nodes("span.a-size-base") %>% 
  html_nodes("div.a-row.a-size-small") %>% 
  html_text() -> amazon_rate_n
amazon_rate_n <- str_trim(amazon_rate_n) # trim
#amazon_rate_n <- str_sub(amazon_rate_n, regex(?<=\s))
amazon_rate_n <- str_sub(amazon_rate_n, -5) # keep last 5 characters
amazon_rate_n <- str_trim(amazon_rate_n) # trim leading spaces
amazon_rate_n <- as.numeric(amazon_rate_n)
head(amazon_rate_n)
```

```{r rate_n_missing, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_rate_n %>% 
  append(values = NA, after = 1) %>% 
  append(values = NA, after = 11) -> amazon_rate_n
head(amazon_rate_n)
```





# Get the book publication date

```{r pub_dt, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon %>% 
  html_nodes("span.a-size-base.a-color-secondary.a-text-normal") %>% 
  html_text() -> amazon_pub_dt
head(amazon_pub_dt)
```

We need to convert this to a date to allow easier analysis.

```{r pub_dt_clean, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
amazon_pub_dt <- as.Date(amazon_pub_dt, "%b %d, %Y")
head(amazon_pub_dt)
```





# We Have the Pieces

Let's assemble the file

### Assemble the data

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#r_books <- tibble(title = amazon_titles,
#                  author = amazon_authors,
#                  text.format = amazon_format,
#                  price = amazon_price,
#                  rating = amazon_rating,
#                  num.ratings = amazon_rate_n,
#                  publication.date = amazon_pub_dt)
#head(r_books)
```





# Thank you
<img src = "images/twitter.png" width="30" height="30">  
[\@mjhendrickson](https://twitter.com/mjhendrickson)

<img src = "images/linkedin.png" width="30" height="30">  
[matthewjhendrickson](https://www.linkedin.com/in/matthewjhendrickson/)

<img src = "images/github.png" width="30" height="30">  
[mjhendrickson](https://github.com/mjhendrickson)

[rtweet repo](https://github.com/mjhendrickson/rtweet-Exploration)

This talk is freely distributed under the MIT License.  
(So is rtweet!)





# References

- Bauer V (2016). "Introduction to Web Scraping in R: Very Applied Methods Workgoup." [https://stanford.edu/~vbauer/files/teaching/VAMScrapingSlides.html](https://stanford.edu/~vbauer/files/teaching/VAMScrapingSlides.html).
- University of Cincinnati (2018). "UC Business Analytics R Programming Guide." Specifically the portion of scraping. [https://uc-r.github.io/](https://uc-r.github.io/).
- Dataquest (no date). "Tutorial: Web Scraping in R with rvest." [https://www.dataquest.io/blog/web-scraping-in-r-rvest/](https://www.dataquest.io/blog/web-scraping-in-r-rvest/).
- Im J (2019). "Web Scraping Product Data in R with rvest and purrr." [https://www.business-science.io/code-tools/2019/10/07/rvest-web-scraping.html](https://www.business-science.io/code-tools/2019/10/07/rvest-web-scraping.html).
- Kaushik S (2017). "Beginner's Guide on Web Scraping in R (using rvest) with hands-on example." [https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/](https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/).
- Perceptive Analytics on KDnuggets (2018). "A Primer on Web Scraping in R." [https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html](https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html).
- Rsquared Academy (2019). "Practical Introduction to Web Scraping in R." [https://blog.rsquaredacademy.com/2019/04/11/web-scraping/](https://blog.rsquaredacademy.com/2019/04/11/web-scraping/).
- W3Schools (no date). "CSS Introduction." [https://www.w3schools.com/css/css_intro.asp](https://www.w3schools.com/css/css_intro.asp).
- W3Schools (no date). "HTML Introduction." [https://www.w3schools.com/html/html_intro.asp](https://www.w3schools.com/html/html_intro.asp).
- Wikipedia (2020). "Web scraping." [https://en.wikipedia.org/wiki/Web_scraping](https://en.wikipedia.org/wiki/Web_scraping).